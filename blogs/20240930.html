<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Define title -->
    <title> [Study Notes] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks </title>

    <!-- Include favicon. Use, e.g., https://favicon.io/ -->
    <link rel="icon" type="image/png" href="../favicon.png" sizes="32x32" />

    <!-- Include style sheets -->
    <link rel="stylesheet" href="../main.css" />
    <link rel="stylesheet" href="../blog.css" />

    <!-- Fonts from Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@300;600&display=swap" rel="stylesheet" />

    <!-- Load FontAwesome (for icons) -->
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"
        integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g=="
        crossorigin="anonymous" />

    <!-- Load Academicons (for more icons) -->
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.2/css/academicons.min.css"
        integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg=="
        crossorigin="anonymous" />

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <!-- Navigation Bar -->
    <nav>
      <ul class="navbar">
          <li><a href="../index.html">Home</a></li>
          <li><a href="../publication.html">Publications</a></li>
          <li><a href="../cv.html">CV</a></li>
          <li><a href="../blogs.html">Blogs</a></li>
      </ul>
    </nav>

    <!-- Main content -->
    <section id="content"> 
        <h2> [Study Notes] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</h2> 
        <i> September 30, 2024 </i>

        <p>
            This blog summarizes my notes on meta-learning and one of the classical meta-learning strategies, 
            Model-Agnostic Meta-Learning (MAML) <a href="#maml">[1]</a>, 
            covering its motivation, methodology, and experiments. 

            I appreciate the online course by Hung-yi Lee <a href="#Hung-yi">[2]</a> for providing a comprehensive overview of meta-learning.
        </p>

        <h3>1. Challenges in bioinformatics and chemical informatics problems</h3>
        <p>
            Traditional machine learning models require large amounts of data for specific tasks, 
            which isn't feasible for all problems. 
            For instance, in bioinformatics and chemical informatics, 
            we often encounter various experimental conditions that may come from different groups or different projects. 
            However, for each condition, 
            we may have only a small amount of data due to limitations in labor, budget, and time. 
            Meta-learning can potentially address these challenges by training models to quickly adapt to new tasks with minimal data.
        </p>

        <h3>2. What is Meta-Learning?</h3>
        <p>
            Meta-learning, often referred to as <highlight>"learning to learn,"</highlight> 
            is an approach that not only learns the model parameters \(f_\theta\)
            but also the training algorithm \(F_\phi\) as show in the following figure. 
        </p>
        <figure>
            <img src="./20240930/meta_learning.png" alt="Meta-Learning Concept" />
            <figcaption>Meta-Learning Concept.</figcaption>
        </figure>
        <p>
            To achieve this goal, <highlight>multiple tasks</highlight> are needed for meta-learning. 
            These tasks are divided into training tasks (e.g., task 1 and task 2 in the figure above) and test tasks. 
            During training, components of learning algorithms are learned across training tasks. 
            Evaluating meta-learning methods involves training models on test tasks, 
            which differs significantly from traditional machine learning methods.
        </p>
        <p>
            It's important to note that meta-learning is <highlight>not</highlight> the same as few-shot learning. 
            While few-shot learning involves using a small amount of data to train a model, 
            meta-learning enables training a model on limited data by training it across multiple tasks.
        </p>

        <p>
            Various components in a learning algorithm can be learned, including:
        </p>
        <ul>
            <li>Data preprocessing: Differentiable automatic data augmentation <a href="#li2020differentiable">[3]</a>; 
                Meta-weight-net: Learning an explicit mapping for sample weighting <a href="#shu2019meta">[4]</a></li>
            <li>Model architecture: Network architecture search <a href="#weng2020neural">[5]</a></li>
            <li>Optimizer (learning rates): Learning to learn by gradient descent by gradient descent <a href="#andrychowicz2016learning">[6]</a></li>
            <li>Initialized parameters: Model-Agnostic Meta-Learning (MAML) <a href="maml">[1]</a></li>
        </ul>

        <h3>3. Model-Agnostic Meta-Learning (MAML)</h3>
        <h4>3.1 Methodology</h4>
        <p>
            Let's refresh the aim of MAML that we wanna find an initialized weights \(\phi\) that when applying on test tasks,
            the model could perform well. 
            Then the loss function for MAML can be defined as follows:
            \[
            L(\phi) = \sum^N_{n=1}l^n(\hat{\theta}^n)
            \]
            where \(\phi\) represents the model's initial weights, 
        \(\hat{\theta}^n\) represents the model's weights learned from task \(n\), 
        and \(l^n(\hat{\theta}^n)\) is the loss on the test set of task \(n\).
        </p>
        <p> 
        The model's weights \(\hat{\theta}^n\) can be learned through gradient descent as tranditional machine learning do. 
        We always contrain it as a one-step gradient update:
            \[
            \hat{\theta}^n = \phi - \alpha {\nabla}_{\phi}l^n(\phi)
            \]
            where \(\alpha\) is the learning rate.
        </p>
        <p>
            How can we minimize \(L(\phi)\)? <highlight>Gradient descent!</highlight>
            \[
            \phi \leftarrow \phi - \beta {\nabla}_{\phi}L(\phi)
            \]
            where \(\beta\) is the learning rate for the meta-learner.
        </p>
        <p>
            Since we already have the definition of \(L(\phi)\), we can replace it in the previous equation: 
            \[
            \phi \leftarrow \phi - \beta {\nabla}_{\phi}\sum^N_{n=1}l^n(\hat{\theta}^n)
            \]

            Using a one-step gradient update to train the model's parameter \(\hat{\theta}^n\), we arrive at the final update rule for MAML:
            \[
            \phi \leftarrow \phi - \beta {\nabla}_{\phi}\sum^N_{n=1}l^n(\colorbox{yellow}{$\phi - \alpha {\nabla}_{\phi}l^n(\phi)$})
            \]
            We refer to the yellow part as the inner loop and the entire process as the outer loop. 
            Typically, we split the data from training tasks into two parts: 
            one for the inner loop and the other for the outer loop. 
            Since the inner loop is a one-step gradient update, 
            it is computationally efficient and requires fewer data points; it can even be performed using \(k\)-shot learning.
        </p>

        
        <h4>3.2 MAML vs. Pretraining</h4>
        <p>
            As both MAML and pretraining aim to find a good initialization for the model, what is the difference between them?    
        </p>
        <p>
            In MAML, the goal is to find \(\phi\) that achieves good performance after training, 
            expressed as:
            \[
            L(\phi) = \sum^N_{n=1}l^n(\hat{\theta}^n)
            \]
            As shown in the figure below, through MAML, we can find initialized weights \(\phi\) 
            that perform well after fine-tuning, even if \(\phi\) does not achieve the best performance initially.
        </p>
        <figure>
            <img src="./20240930/meta_learning_loss.png" alt="MAML Loss" style="width:60%;"/>
            <figcaption>Loss in MAML. Image sourced from: the online course by Hung-yi Lee <a href="#Hung-yi">[2]</a></figcaption>
        </figure>
        <p>
            In contrast, pretraining focuses on finding \(\phi\) that achieves good performance without training specific tasks:
            \[
            L(\phi) = \sum^N_{n=1}l^n(\phi)
            \]
            In the figure below, \(\phi\) initially performs well on both task \(1\) and task \(2\), 
            but after fine-tuning, \(l^2(\hat{\theta}^2)\) converges to a locally optimized point,
            which is not as optimal as the result achieved by MAML. 
        </p>
        <figure>
            <img src="./20240930/pretraining_loss.png" alt="Pretraining Loss" style="width:60%;"/>
            <figcaption>Loss in Pretraining. Image sourced from: the online course by Hung-yi Lee <a href="#Hung-yi">[2]</a></figcaption>
        </figure>

        <h4>3.3 Experiments: regression on Sine wave</h4>
        <p>
            In a toy example, we define a task \(y = a sin(x + b)\). 
            We sample \(k\) points from the target function to estimate it, 
            The tasks are formed by sampling \(a\) and \(b\).
        </p>
        <p>
            The following figures show the randomly generated 3 and 1000 tasks, respectively. 
            You may already notice the problem: 
            if we train a model on all 1000 tasks together, 
            it would converge to the average of all tasks, which is zero.
        </p>
        <figure>
            <img src="./20240930/maml_toy_example.png" alt="MAML Toy Example" />
            <figcaption>Regression tasks establishment. Images sourced from: <a href="#paper_repro">[7]</a></figcaption>
        </figure>
        <p>
            Incidentally, this is a toy example to illustrate the issue clearly. 
            In real-world applications, 
            this problem may not be as obvious. 
            However, the key takeaway is that when training a model on multiple tasks, 
            we need to consider whether the diversity of tasks negatively impacts individual task performance.
        </p>

        <p>
            The following figures show the performance of fine-tuning models from MAML and pretraining on sine wave simulation.
            As we expected, the pretrained model is initialized at all zeros. 
            After fine-tuning on 10 instances, it cannot converge. 
            However, the model initialized by MAML does converge.
        </p>
        <figure>
            <img src="./20240930/maml_regression.png" alt="MAML Regression" />
            <figcaption>Fine-tuning for regression tasks from pretrained models and MAML. Images sourced from: <a href="#paper_repro">[7]</a></figcaption>
        </figure>

        <h4>3.4 Experiments: classification</h4>
        <p>
            The authors took \(5\)-way \(1\)-shot and \(5\)-way \(5\)-shot evaluations on MiniImagenet dataset.
            This dataset is commonly used as a few-shot learning benchmark, 
            involving 52 classes. 
            The \(N\)-way classification problem is set up by selecting \(N\) unseen classes and providing \(K\) instances of each class. 
            The model is then fine-tuned on these instances to classify the remaining instances from those classes.
        </p>
        <figure>
            <img src="./20240930/maml_classification.png" alt="MAML Classification" />
            <figcaption>Evaluation on classification tasks. Images sourced from: Model-agnostic meta-learning for fast adaptation of deep networks <a href="#maml">[1]</a></figcaption>
        </figure>


        <h3>Following Works</h3>
        <p>
            MAML marks the start of meta-learning for initialization. 
            It faces challenges such as sensitivity to task diversity, 
            dependence on adequate training tasks, 
            and difficulties in scaling to complex models. 
            Subsequent research efforts aim to address these issues: 
        </p>
        <ul>
            <li>[ANIL, Almost No Inner Loop] Rapid learning or feature reuse? Towards understanding the effectiveness of MAML
                <a href="#anil">[8]</a>.</li>
            <li>[MAML++] How to train your MAML
                <a href="#maml_pp">[9]</a>.</li>
            <li>[Reptile] On first-order meta-learning algorithms
                <a href="#reptile">[10]</a>.
                For more information, visit <a href="https://openai.com/index/reptile/">OpenAI's Reptile page</a>.
            </li>
        </ul>

        <h2>References</h2>
        <ol class="custom-counter">
            <li id="maml">Finn, C., et al. (2017). <em>Model-agnostic meta-learning for fast adaptation of deep networks</em>. ICML</li>
            <li id="Hung-yi">Hungyi, L. (2021). <a href="https://www.youtube.com/watch?v=FmJ4T4k88jY">Lecture 37: Meta Learning</a></li>
            <li id="li2020differentiable">Li, Y., et al. (2020). <em>Differentiable automatic data augmentation</em>. ECCV</li>
            <li id="shu2019meta">Shu, J., et al. (2019). <em>Meta-weight-net: Learning an explicit mapping for sample weighting</em>. NeurIPS</li>
            <li id="weng2020neural">Weng, L. (2020). <a href="https://lilianweng.github.io/posts/2020-08-06-nas/">Neural Architecture Search</a>.</li>
            <li id="andrychowicz2016learning">Andrychowicz, M., et al. (2016). <em>Learning to learn by gradient descent by gradient descent</em>. NeurIPS</li>
            <li id="paper_repro"> Adrien, E. (2018). <a href="https://towardsdatascience.com/paper-repro-deep-metalearning-using-maml-and-reptile-fd1df1cc81b0">Paper repro: Deep Metalearning using “MAML” and “Reptile”</a></li>
            <li id="anil">Antoniou, A., et al. (2019). <em>How to train your MAML</em>. ICLR.</li>
            <li id="maml_pp">Raghu, A., et al. (2019). <em>Rapid learning or feature reuse? Towards understanding the effectiveness of MAML</em>. ICLR.</li>
            <li id="reptile">Nichol, A. (2018). <em>On first-order meta-learning algorithms</em>. arXiv preprint arXiv:1803.02999.</li>
        </ol>
        
        <footer>
            <p>&copy; 2024 Yuhui Hong. All rights reserved.</p>
        </footer>
    </section>
    
</body>

</html>
